{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen-Image-Edit - Pruebas Locales\n",
        "\n",
        "Este notebook te permite probar Qwen-Image-Edit localmente sin desplegar en RunPod.\n",
        "\n",
        "## Requisitos:\n",
        "- GPU con al menos 20GB VRAM\n",
        "- Docker con soporte NVIDIA\n",
        "- Contenedor ejecutándose localmente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar GPU disponible\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(\"=== VERIFICACIÓN DE HARDWARE ===\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"VRAM libre: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
        "\n",
        "# Verificar memoria del sistema\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,noheader,nounits'], \n",
        "                          capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        total, used, free = result.stdout.strip().split(', ')\n",
        "        print(f\"\\nNVIDIA-SMI:\")\n",
        "        print(f\"  Total: {int(total)} MB\")\n",
        "        print(f\"  Usado: {int(used)} MB\")\n",
        "        print(f\"  Libre: {int(free)} MB\")\n",
        "except:\n",
        "    print(\"No se pudo obtener información de nvidia-smi\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar Qwen-Image-Edit Pipeline\n",
        "import time\n",
        "from diffusers import QwenImageEditPipeline\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"=== CARGANDO QWEN-IMAGE-EDIT ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Cargar el pipeline\n",
        "    pipeline = QwenImageEditPipeline.from_pretrained(\n",
        "        \"Qwen/Qwen-Image-Edit\",\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        use_safetensors=True\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    # Optimizaciones de memoria\n",
        "    pipeline.enable_memory_efficient_attention()\n",
        "    pipeline.enable_model_cpu_offload()\n",
        "    \n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"✅ Pipeline cargado exitosamente en {load_time:.2f} segundos\")\n",
        "    \n",
        "    # Verificar memoria después de la carga\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"VRAM usada: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB\")\n",
        "        print(f\"VRAM reservada: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error cargando el pipeline: {str(e)}\")\n",
        "    print(\"Verifica que tienes suficiente VRAM (mínimo 20GB)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
